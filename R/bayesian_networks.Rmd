---
title: "Bayesian Networks"
author: "Verena Brufatto"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

.libPaths(new = "/packages")
BiocManager::install('gRain')
BiocManager::install("gRbase")
BiocManager::install("igraph")
BiocManager::install("pcalg")

library("gRain")
library("gRbase")
library("igraph")
library("pcalg")
library("BiocManager")
library('bnlearn')
library('graph')
library('Rgraphviz')
library("grid")
library(Hmisc)
library(ggm)
library(kableExtra)

data(cad1)

```

# Dataset description

This contains data on coronary artery disease from a Danish heart clinic.
The dataset contains 14 discrete variables recorded for 236 patients, 107 of whom actually had the disease. The dataset includes five background variables (sex, hypercholesterolemia, smoking, heridary disposition
and workload), one recording whether or not the patient has coronary artery disease,
four variables representing disease manifestation (hypertrophy, previous myocardial
infarct, angina pectoris, other heartfailures), and four clinical measurements (Qwave, T-wave, Q-wave informative and T-wave informative). Angina pectoris has 3
levels and the remaining 13 variables are binary.


```{r, eval=T, echo=F, include=T}

describe(cad1)

```


# Blacklist

A better approach is to incorporate our prior knowledge of the system under
study into the model selection process. 

The variables are divided into four blocks, namely background variables, disease (which includes CAD and hypertrophy), disease manifestations and clinical measurements. We restrict the model selection
process by blacklisting arcs that point from a later to an earlier
block. We also blacklist arcs that point from being a smoker to having an hereditary disposition and from smoker, hypercholesterolemia, hereditary and workload to sex. 
In addition, we assume independence between different clincal tests, so that STchange and STcode cannot influence QWave and QWavecode and viceversa. Finally, we impose that the confidence on a test result (STcode and QWavecode) cannot influence whether or not a test is run (STchange, QWave).

```{r, eval=T, echo=F, include=T}

block <- c(1,3,3,4,4,4,4,1,2,1,1,1,3,2) # assign group to each variable

blM <- matrix(0, nrow=14, ncol=14) # define forbidden edges
rownames(blM) <- colnames(blM) <- names(cad1)

for (b in 2:4) blM[block==b, block<b] <- 1 # for b = 2, edges from group2 (rows) to group1 (cols)

blM["Smoker", "Inherit"] <- 1
blM[c("Smoker", "Hyperchol", "Inherit", "SuffHeartF"), "Sex"] <- 1
blM[c("QWave", "QWavecode"), c("STchange", "STcode")] <- 1
blM[c("STchange", "STcode"), c("QWave", "QWavecode")] <- 1
blM["STcode", "STchange"] <- 1
blM["QWavecode", "QWave"] <- 1

blackL <- data.frame(get.edgelist(as(blM, "igraph"))) # get list of forbidden edges
names(blackL) <- c("from", "to")

blackL
```

# Learn network structure

## True CPDAG

We assume that the true CPDAG is the one described in Hojsgaard and Thiesson (1993). 
The true CPDAG has 19 arcs of which 2 undirected. 

```{r, eval=T, echo=F, include=F}

true <- matrix(0, nrow=14, ncol=14) 
rownames(true) <- colnames(true) <- names(cad1)

true["Sex", "Smoker"] = 1
true["Smoker", "Hyperchol"] = 1
true["Hyperchol", "CAD"] = 1
true["Inherit", "SuffHeartF"] = 1
true["SuffHeartF", c("Inherit", "Hypertrophi", "CAD", "AMI")] = 1
true["AMI", c("SuffHeartF", "QWavecode", "STchange")] = 1
true["CAD", c("AngPec", "QWavecode", "STcode", "STchange")] = 1
true["Hypertrophi", c("Heartfail", "STcode", "STchange")] = 1
true["AngPec", "QWave"] = 1
true["QWave", "QWavecode"] = 1
true["STchange", "STcode"] = 1

true_dag = empty.graph(names(cad1))
amat(true_dag) <- true

narcs(true_dag) 
undirected.arcs(true_dag)

```

```{r, eval=T, echo=F, include=T}

graphviz.plot(true_dag)

```

## Score based algorithms

### Hill climbing algorithm

The hill climbing algorithm produces a network with 19 directed arcs.

```{r, eval=T, echo=T, include=T}

bn_hc <- hc(cad1, blacklist=blackL) 

```

```{r, eval=T, echo=F, include=T}

graphviz.plot(bn_hc)

na = narcs(bn_hc)

```


```{r, eval=T, echo=T, include=T}

unlist( bnlearn::compare(true_dag, cpdag(bn_hc)) )

```

```{r, eval=T, echo=F, include=T}

par(mfrow = c(1, 2))
graphviz.compare(true_dag, cpdag(bn_hc))

```

### Hill climbing algorithm with random restarts

The algorithm produces a network with 21 directed arcs. 

```{r, eval=T, echo=T, include=T}

hc_restart = hc(cad1, score = "bde", iss = 1, restart = 10, perturb = 5, blacklist=blackL) 


```

```{r, eval=T, echo=F, include=T}

graphviz.plot(hc_restart)

na = narcs(hc_restart)

```

```{r, eval=T, echo=T, include=T}

# compare score based algorithms

all.equal(bn_hc, hc_restart)

```

```{r, eval=T, echo=T, include=T}

unlist( bnlearn::compare(bn_hc, hc_restart) )

```

```{r, eval=T, echo=F, include=T}

par(mfrow = c(1, 2))
graphviz.compare(bn_hc, hc_restart)

```

Based on the network score, the DAG produced with the hill climbing algorithm with random restarts provides a slightly worse fit to the data. 

```{r, eval=T, echo=F, include=T}

s1 <- bnlearn::score(bn_hc, data = cad1, type = "bic")

s2 <- bnlearn::score(hc_restart, data = cad1, type = "bic")

c(s1, s2)

```

```{r, eval=T, echo=T, include=T}

# compare with true dag
unlist( bnlearn::compare(true_dag, cpdag(hc_restart)) )

```

```{r, eval=T, echo=F, include=T}

par(mfrow = c(1, 2))
graphviz.compare(true_dag, cpdag(hc_restart))

```

## Constraint based algorithms

###  Semi-Interleaved Hiton-PC

The algorihm produces a network with 10 arcs, 1 of which undirected. 
```{r, eval=T, echo=T, include=T}

bn_hit <- si.hiton.pc(cad1, undirected = F, 
                      blacklist = blackL)

```

```{r, eval=T, echo=F, include=T}

graphviz.plot(bn_hit)

na = narcs(bn_hit)
nund = undirected.arcs(bn_hit)

```

```{r, eval=T, echo=T, include=T}

unlist( bnlearn::compare(true_dag, bn_hit) )

```

```{r, eval=T, echo=F, include=T}

par(mfrow = c(1, 2))
graphviz.compare(true_dag, bn_hit)

```

### Hiton-PC with permutation tests

The network learned has the same number of directed and undirected arcs as the previous one but a different arc set (2 false positives and 1 false negative). 

```{r, eval=T, echo=T, include=T}

bn_hit2 = si.hiton.pc(cad1, test = "mc-mi", undirected = FALSE,
                      blacklist = blackL)


```

```{r, eval=T, echo=F, include=T}

graphviz.plot(bn_hit2)

na = narcs(bn_hit2)
nund = undirected.arcs(bn_hit2)

```


```{r, eval=T, echo=T, include=T}

unlist( bnlearn::compare(bn_hit, bn_hit2) )

```

```{r, eval=T, echo=F, include=T}

par(mfrow = c(1, 2))
graphviz.compare(bn_hit, bn_hit2)

```

```{r, eval=T, echo=T, include=T}

unlist( bnlearn::compare(true_dag, bn_hit2) )

```

```{r, eval=T, echo=F, include=T}

par(mfrow = c(1, 2))
graphviz.compare(true_dag, bn_hit2)

```

## Hybrid algorithms

```{r, eval=T, echo=T, include=T}

bn_rsmax <- rsmax2(cad1, restrict="mmpc", maximize="hc", blacklist = blackL)


```

```{r, eval=T, echo=F, include=T}

graphviz.plot(bn_rsmax)

```

```{r, eval=T, echo=T, include=T}

unlist( bnlearn::compare(true_dag, cpdag( bn_rsmax)) )

```

```{r, eval=T, echo=F, include=T}

par(mfrow = c(1, 2))
graphviz.compare(true_dag, cpdag(bn_rsmax))

```

## Model averaging

### Frequentist approach

We average multiple networks to improve the structure learned. Bootstrap resampling is applied to the dataset to learn a set of 500 CPDAGS with the hill climbing algorithm. Arcs are considered significant if they appear in at least 85% of the networks,
and in the direction that appears most frequently.

```{r, eval=T, echo=T, include=T}

boot <- boot.strength(cad1, R = 500, algorithm = "hc",
                      algorithm.args = list(score = "bde", iss = 10))

boot[(boot$strength > 0.85) & (boot$direction >= 0.5), ]



```

Since all the values in the direction column are well above 0.5, we can infer that the direction of the arcs is well established and that they are not score equivalent. Lowering the threshold
from 85% to 50% does not change the results of the analysis, which seems to
indicate that in this case the results are not sensitive to its value.

```{r, eval=T, echo=T, include=T}

avg.boot <- averaged.network(boot, threshold = 0.85)

```

```{r, eval=T, echo=F, include=T}

graphviz.plot(avg.boot)

```

```{r, eval=T, echo=T, include=T}

unlist( bnlearn::compare(true_dag, cpdag( avg.boot)) )

```

```{r, eval=T, echo=F, include=T}

par(mfrow = c(1, 2))
graphviz.compare(true_dag, cpdag(avg.boot))

```

# Model comparison

There are 5 archs that appear both in the true DAG and in the DAG learned by the Hill Climbing algorithm and 14 arcs that appear in the current DAG but not in the true DAG and viceversa. 

```{r, eval=T, echo=F, include=F}

# compare CPDAGS
nws <- list(hc_restart, bn_hc, bn_hit,
            bn_hit2, bn_rsmax, avg.boot)

class <- c("d", "d", "c", "c", "d", "d")

results <- matrix(0, ncol = 3, nrow = length(nws))

for(i in 1:length(nws)){
  
  if(class[i] == "d"){current <- cpdag(nws[[i]])} else { current <- nws[[i]]}
  
  results[i, ] <- unlist( bnlearn::compare(true_dag,current) )
}

colnames(results) <- c("tp", "fp", "fn")
rownames(results) <- c("HC restarts", "HC", "Hiton", "Hiton permutations", 
                       "Hybrid", "Averaging")

```

```{r, eval=T, echo=F, include=T}

results %>%
  kbl(caption = "Compare CPDAGS") %>%
  kable_styling() %>%
  row_spec(2, bold = T, color = "red") 

```


```{r, eval=T, echo=F, include=F}

# compare skeletons
nws <- list(hc_restart, bn_hc, bn_hit,
            bn_hit2, bn_rsmax, avg.boot)

results <- matrix(0, ncol = 3, nrow = length(nws))

for(i in 1:length(nws)){
  
  results[i, ] <- unlist( bnlearn::compare(bnlearn::skeleton(true_dag), bnlearn::skeleton(nws[[i]])) )
}

colnames(results) <- c("tp", "fp", "fn")
rownames(results) <- c("HC restarts", "HC", "Hiton", "Hiton permutations", 
                       "Hybrid", "Averaging")

```

```{r, eval=T, echo=F, include=T}

results %>%
  kbl(caption = "Compare skeletons") %>%
  kable_styling() 

```


The skeleton of the network learned by the HC algorithm differs from the skeleton of the true DAG by 21 arcs, while its CPDAG differs from the true CPDAG by 23 arcs. 


```{r, eval=T, echo=F, include=T}
## hamming distance
nws <- list(hc_restart, bn_hc, bn_hit,
            bn_hit2, bn_rsmax, avg.boot)

hd <- matrix(0, nrow = length(nws), ncol = 2)

for(i in 1:length(nws)){
  
  if(class[i] == "d"){current <- cpdag(nws[[i]])} else { current <- nws[[i]]}
  
  hamm <- hamming(true_dag, nws[[i]])
  strhamm <- bnlearn::shd(true_dag, current)
  
  hd[i, ] <- c(hamm, strhamm)
}

colnames(hd) <- c("Hamming distance", "SHD")
rownames(hd) <- c("HC restarts", "HC", "Hiton", "Hiton permutations", 
                       "Hybrid", "Averaging")

```

```{r, eval=T, echo=F, include=T}

hd %>%
  kbl(caption = "Hamming distance") %>%
  kable_styling() 

```


# Parameter estimate 

We choose to use the Bayesian network produced by the hill climbing algorithm for the purpose of statistical inference. The graph is chosen because it has the largest number of true positive arcs with respect to the true CPDAG among the estimated graphs. The networks produced by the constraint based algorithms and by the hybrid algorithms are not considered eligible for selection because the node CAD has no parents, making it impossible to inquire about the underlying causes of the disease. 

The joint probability distribution of the model obtained with the hill climbing algorithm factorizes according to: 

P(Sex, AngPec, AMI, QWave, QWavecode,STcode, STchange, SuffHeartF, Hypertrophi, Hyperchol,   Smoker,      Inherit,    
Heartfail,   CAD ) = 
P(Sex)P(Inherit)P(SuffHeartF)P(Smoker|Sex)P(AMI|CAD)P(CAD| Inherit, Hyperchol)
P(Hyperchol|Inherit, SuffHeartF) P(QWave|AMI, CAD) P(STchange| CAD, Hypertrophi)
P(AngPec|CAD) P(Hypertrophi| CAD, SuffHeartF) P(Heartfail|Hypertrophi)
P(QWavecode|Hypertrophi, SuffHeartF) P(STcode|STchange, SuffHeartF)

The MLE and the Bayes estimator yield similar results for the model coefficients.  

```{r, eval=T, echo=T, include=T}

fit_mle = bn.fit(bn_hc, cad1, method ="mle") 

coef(fit_mle$CAD)


```

```{r, eval=T, echo=T, include=T}

fit_bay = bn.fit(bn_hc, cad1, method = "bayes", iss = 10)

coef(fit_bay$CAD)

```



# Inference 

## Exact inference

```{r, eval=T, echo=F, include=T, fig.cap= "Bayesian network"}

graphviz.plot(bn_hc)

```

```{r, eval=T, echo=F, include=T, fig.cap= "Junction tree"}

cad_gr <- as(amat(bn_hc), "graphNEL")
cad_grain <- grain(cad_gr, data=cad1)

junction <- compile(cad_grain)

plot(junction,type="jt",main="Junction Tree")

```

```{r, eval=T, echo=T, include=T}

querygrain(junction, nodes = "CAD")

```

Since smoker is not directly connected to CAD in the graph, we find that $P(CAD| Smoker = Yes) = P(CAD)$.

```{r, eval=T, echo=T, include=T}

gr1.ev1 <- setFinding(junction,nodes=c("Smoker"),states=list(c("Yes")))
querygrain(gr1.ev1,nodes=c("CAD"),type = "marginal")

```

In fact, the two nodes are d-separated in the original graph.

```{r, eval=T, echo=T, include=T}

bnlearn::dsep(bn_hc, x = "CAD", y = "Smoker")

```

The probability of observing $CAD = Yes$ is higher for the evidence $Inherit = Yes$ (about 66%) since the two nodes are connected by an arc. 

```{r, eval=T, echo=T, include=T}

gr1.ev1 <- setFinding(junction,nodes=c("Inherit"),states=list(c("Yes")))
querygrain(gr1.ev1,nodes=c("CAD"),type = "marginal")

```

If both Inherit and Hypercol are present, the probability of CAD is about 74%

```{r, eval=T, echo=T, include=T}

gr1.ev2 <- setFinding(junction, nodes=c("Inherit", "Hyperchol"),states=list(c("Yes"), c( "Yes" )))
querygrain(gr1.ev2,nodes=c("CAD"),type = "marginal")

```

Adding SuffHeart to the evidence does not change the probability distribution of CAD

```{r, eval=T, echo=T, include=T}

gr1.ev3 <- setFinding(junction, nodes=c("Inherit", "Hyperchol", "SuffHeartF"),states=list(c("Yes"), c( "Yes" ), c( "Yes" )))
querygrain(gr1.ev3,nodes=c("CAD"),type = "marginal")

```

```{r, eval=T, echo=T, include=T}

gr1.ev4 <- setFinding(junction, nodes=c( "SuffHeartF"),states=list(c("Yes")))
querygrain(gr1.ev4,nodes=c("CAD"),type = "marginal")

```

However, the two nodes are not d-separated in the orignal graph, since there is a path going from SuffHeartF to CAD.

```{r, eval=T, echo=T, include=T}

bnlearn::dsep(bn_hc, x = "SuffHeartF", y = "CAD")

```

```{r, eval=T, echo=T, include=T}

bnlearn::path(bn_hc, from = "SuffHeartF", to = "CAD")

```

SuffHeartF and CAD are not conditionally independent even if we condition on Hyperchol, since there exist other paths between the nodes. 

```{r, eval=T, echo=T, include=T}

bnlearn::dsep(bn_hc, x = "SuffHeartF", y = "CAD", z = "Hyperchol")

```
The presence of CAD makes AngPec and QWave more likely and Hypertrophi less likely

```{r, eval=T, echo=T, include=T}

gr1.ev5 <- setFinding(junction, nodes=c( "CAD"),states=list(c("Yes")))
querygrain(gr1.ev5,nodes=c("AngPec", "Hypertrophi", "QWave"),type = "marginal")

```

Next, we investigate the joint distribution of CAD and Hyperchol given that the patient has an hereditary predisposition. 

```{r, eval=T, echo=T, include=T}

gr1.ev6 <- setFinding(junction, nodes=c(  "Inherit"), states=list(c("Yes")))
querygrain(gr1.ev6, nodes=c("CAD", "Hyperchol"), type = "joint")

```

Finally, we compute the probability of CAD conditional on the evidence and the values of Hyperchol $P(CAD| Inherit = Yes, Hypercol = h), h \in \{ Yes, No \}$. 

```{r, eval=T, echo=T, include=T}

querygrain(gr1.ev6, nodes = c("CAD", "Hyperchol"), type = "conditional")

```


## Approximate inference

### Logic sampling

By generating $10^6$ observations from the fitted Bayesian network we find that $P(CAD = Yes | Inherit = Yes) = 0.65$ and $P(CAD = Yes | Hyperchol = Yes, Inherit = Yes) = 0.73$, in line with the previous findings obtained via exact inference. In both queries, $P(E)$ is large enough for the logic sampling algorithm to estimate the probability with sufficient precision, which is confirmed by the fact that the likelihood weighting approach yields similar results.

```{r, eval=T, echo=T, include=T}

cpquery(fit_bay, event=(CAD=="Yes"),
evidence = (Inherit=="Yes"), method="ls", n=10^6)

```

```{r, eval=T, echo=T, include=T}

cpquery(fit_bay, event=(CAD=="Yes"),
evidence = (Inherit=="Yes") & (Hyperchol == "Yes"), method="ls", n=10^6)

```

```{r, eval=T, echo=T, include=T}

cpquery(fit_bay, event=(CAD=="Yes"),
evidence = list(Inherit ="Yes", Hyperchol = "Yes"), method="lw")

```

# Classification

## Naive Bayes Classifier

The naive Bayes classifier predicts the correct class about 85% of the time, whereas the original Bayesian network predicts the correct class only 67% of the time. Generating a random sample, as we did in this example, is optional since the database is large enough for the classifier to yield robust estimates even if applied to the original data. 

```{r, eval=T, echo=T, include=T}

set.seed(123)

survey <- bnlearn::rbn(fit_bay,1000)
nbcl <- naive.bayes (survey, training="CAD")
graphviz.plot(nbcl,layout="fdp")

```


```{r, eval=T, echo=T, include=T}

nbcl.trained <- bn.fit(nbcl,survey)

coef(nbcl.trained$CAD)

```


```{r, eval=T, echo=T, include=T}

coef(nbcl.trained$Hypertrophi)

```

```{r, eval=T, echo=T, include=T, warning=F}

set.seed(123)

cv.nb <- bn.cv(nbcl,data=survey,runs=10,
method="k-fold",folds=10)
cv.nb

```


```{r, eval=T, echo=T, include=T, warning=F}

set.seed(123)

cv.orig <- bn.cv(bn_hc, data = survey, runs=10, method ="k-fold", folds=10,
                 loss="pred", loss.args = list(target="CAD"))
cv.orig

```

## Tree-Augumented Naive Bayes Classifier

The TAN classifier has a slightly higher predictive accuracy than the Naive Bayes classifier (86%).

```{r, eval=T, echo=T, include=T}

set.seed(123)

tan.cl <- tree.bayes(survey,training="CAD")
graphviz.plot(tan.cl)

```

```{r, eval=T, echo=T, include=T}

tancl.trained <- bn.fit(tan.cl,survey)
coef(tancl.trained$CAD)

```

```{r, eval=T, echo=T, include=T}

tancl.trained <- bn.fit(tan.cl,survey)
coef(tancl.trained$Hypertrophi)

```

```{r, eval=T, echo=T, include=T, warning=F}

set.seed(123)

cv.tan <- bn.cv("tree.bayes",data=survey,
                runs=10,
                method="k-fold",folds=10,algorithm.args = list(training ="CAD"))
cv.tan 

```

By generating 1000 samples from the bayesian network we obtain a distribution with very low variance.  

```{r, eval=T, echo=T, include=T}

plot(cv.orig,cv.nb,cv.tan, xlab=c("SURVEY","NBC","TAN")) 

```